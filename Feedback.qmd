---
title: "Feedback on Your Product and Evaluation Process"
format: html
editor: visual
---

Dear Giskard Team,

I prepared this brief document to share some feedback with you. Overall, I was impressed with the quality of your product and the effort put into the documentation - whoever is responsible for that has my appreciation.

Here are some points that stood out to me during the evaluation:

1.  **Tailoring for Different Skill Levels:**

    -   As I said, I loved the documentation but I found there was a bit of back and forth between different sections. It might be helpful to tailor the documentation to users with varying levels of experience in software development, GitHub, and Python to kind of "direct them" from beginning to end without requiring them to jump between links, which may cause them to lose track in the sequence of tasks. Maybe a very brief questionnaire asking about their experience and resources already installed in their local machine could be helpful. However this is just an idea for improvement and the documentation works fine the way it is currently.

2.  **Concerns about Output Presentation:**

    -   Regarding the output presentation after conducting the model evaluation, I think that the tabular format may be okay for up to five items, but beyond that, it becomes challenging to maintain a holistic view. I will have to think about how to integrate the cells of the table better. Maybe they can be grouped by "query type", with categories such as "normative queries" (asking why A), "contrastive queries" (asking why not B), or "counterfactual queries" (if A is given, asking minimum change required to get B) or something like that.

    -   Additionally, after reviewing the provided examples, I believe there might be an issue with the output. For instance, take a look at the image below. This is taken from the evaluation output in my own technical exercise. The fifth column suggest that the model contradicts itself with respect to the first and the second output. But I am not sure I understand why that is the case. Take a look - isn't the model coherent across the two outputs (i.e., column 2 and 4?).

![Example of potentially incorrect output in the evaluation interface.](OneDrive/Pictures/Catture di schermata/gis-feedback.png)

1.  **Missing Link in Blog Post:**

    -   While going through the evaluation, I noticed a missing link in one of the blog posts: [8 reasons why you need Quality for AI / ML (giskard.ai)](https://www.giskard.ai/knowledge/why-quality-assurance-for-ai). Under the paragraph "Building trust with end-users", the link on "are context-dependent" is broken. It's a small detail, but it could impact the overall user experience.

2.  **Time Estimate for Exercise:**

    -   Finally, another minor point, but I don't think the exercise can be completed in one day if one wants to do a good job. Depending on prior experience, completing parts 1 and 2 could take less than a day, but crafting a quality blog post, might require more than an afternoon. For example this is a blog post I am proud of ([Exploring user interaction challenges with large language models --- Schwartz Reisman Institute (utoronto.ca)](https://srinstitute.utoronto.ca/news/exploring-user-interaction-challenges-with-large-language-models)), but with conceptualization and edits and revisions it took me about a week to write it from start to finish. My humble suggestion is to just give a heads up to applicants that the time required to complete the exercise may vary depending on experience with tools, debugging skills, and speed in producing a piece of content. But again very minor. 10 days are certainly more than enough to do everything.

I hope you find this feedback helpful, and I appreciate the opportunity to share my thoughts. Feel free to reach out if you need any further clarification.

Best regards, and hope to hear from you soon,

Davide Gentile
